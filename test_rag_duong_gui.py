#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test RAG Pipeline with Point Deduction + Qwen 3 Model Integration
Uses rag_pipeline_with_points.py for enhanced point deduction tracking
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from rag_pipeline_with_points import TrafficLawRAGWithPoints
import json


class TrafficLawQAWithPoints:
    """Complete QA system with enhanced RAG + Qwen 3"""
    
    def __init__(self, data_path: str, model_name: str = r"D:\crawl_law\qwen3-0.6B-instruct-trafficlaws\checkpoint-13176"):
        print("Initializing Traffic Law QA System with Point Deduction...")
        
        # Initialize enhanced RAG
        print("\nLoading enhanced RAG pipeline...")
        self.rag = TrafficLawRAGWithPoints(data_path)
        
        # Load model and tokenizer
        print(f"\nLoading model: {model_name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            
            if torch.cuda.is_available():
                print(f"   Model loaded on GPU: {torch.cuda.get_device_name(0)}")
            else:
                print(f"   Model loaded on CPU")
                
        except Exception as e:
            print(f"   Error loading model: {e}")
            print(f"   Trying backup model...")
            model_name = "Qwen/Qwen2.5-0.5B-Instruct"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            print(f"   Loaded backup model: {model_name}")
        
        self.model.eval()
        print("\nSystem ready!")
    
    def format_context(self, retrieval_result: dict) -> str:
        """Format retrieval results into context for the model"""
        
        if retrieval_result["status"] != "success":
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
        
        primary = retrieval_result["primary_chunk"]
        
        # Build context - FOCUS ON PRIMARY ONLY
        context_parts = [
            "=== ƒêI·ªÄU KHO·∫¢N CH√çNH ===",
            f"ƒêi·ªÅu kho·∫£n: {primary['reference']}",
            f"\nN·ªôi dung quy ƒë·ªãnh:",
            primary['content']
        ]
        
        # Add penalty info if available
        if primary.get('penalty'):
            penalty_text = primary['penalty']['text']
            context_parts.append(f"\nM·ª©c ph·∫°t ti·ªÅn: {penalty_text}")
        else:
            context_parts.append(f"\nM·ª©c ph·∫°t: T·ªãch thu ph∆∞∆°ng ti·ªán")
        
        # Add point deduction if available
        if primary.get('point_deduction'):
            context_parts.append(f"Tr·ª´ ƒëi·ªÉm GPLX: {primary['point_deduction']} ƒëi·ªÉm")
        
        # Add license suspension if available
        if primary.get('license_suspension'):
            context_parts.append(f"T∆∞·ªõc GPLX: {primary['license_suspension']['text']}")
        
        return "\n".join(context_parts)
    
    def generate_answer(self, query: str, retrieval_result: dict, max_length: int = 512) -> str:
        """Generate answer using Qwen 3 model"""
        
        # Format context
        context = self.format_context(retrieval_result)
        primary = retrieval_result["primary_chunk"]
        
        # Create prompt emphasizing the primary violation
        system_message = """B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n ph√°p lu·∫≠t giao th√¥ng Vi·ªát Nam.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH√çNH X√ÅC tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

QUY T·∫ÆC B·∫ÆT BU·ªòC:
- PH·∫¢I sao ch√©p CH√çNH X√ÅC s·ªë ti·ªÅn, s·ªë ƒëi·ªÉm, s·ªë th√°ng t·ª´ th√¥ng tin ƒë∆∞·ª£c cung c·∫•p  
- PH·∫¢I n√™u ƒê·∫¶Y ƒê·ª¶: m·ª©c ph·∫°t ti·ªÅn + tr·ª´ ƒëi·ªÉm (n·∫øu c√≥) + t∆∞·ªõc b·∫±ng (n·∫øu c√≥)
- KH√îNG b·ªè s√≥t b·∫•t k·ª≥ h√¨nh ph·∫°t n√†o
- KH√îNG t·ª± √Ω thay ƒë·ªïi con s·ªë
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn 2-3 c√¢u"""
        
        # Build structured answer template based on available info
        answer_parts = []
        if primary.get('penalty'):
            answer_parts.append("m·ª©c ph·∫°t ti·ªÅn")
        if primary.get('point_deduction'):
            answer_parts.append("s·ªë ƒëi·ªÉm b·ªã tr·ª´")
        if primary.get('license_suspension'):
            answer_parts.append("th·ªùi gian t∆∞·ªõc b·∫±ng")
        
        answer_instruction = f"H√£y n√™u r√µ: {', '.join(answer_parts)}."
        
        # Simple, direct prompt that forces copying
        prompt = f"""<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
C√¢u h·ªèi: {query}

{context}

{answer_instruction}<|im_end|>
<|im_start|>assistant
Theo {primary['reference']}:"""
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        # Move inputs to same device as model
        device = next(self.model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate - STRICT PARAMETERS TO PREVENT HALLUCINATION
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,  # Reduced from 512 - shorter answers, less hallucination
                temperature=0.1,     # MUCH lower - almost deterministic
                top_p=0.9,           # Slightly higher to avoid repetition loops
                do_sample=True,
                repetition_penalty=1.05,  # Minimal penalty to preserve numbers
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        
        # Extract only the assistant's response
        if "<|im_start|>assistant" in full_response:
            answer = full_response.split("<|im_start|>assistant")[-1]
            if "<|im_end|>" in answer:
                answer = answer.split("<|im_end|>")[0]
            answer = answer.strip()
        else:
            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if query in answer:
                answer = answer.split(query)[-1].strip()
        
        return answer
    
    def ask(self, query: str, verbose: bool = True) -> dict:
        """Complete QA pipeline: retrieve + generate"""
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"QUESTION: {query}")
            print(f"{'='*80}")
        
        # Step 1: Retrieve
        if verbose:
            print("\n[1/2] Retrieving relevant law provisions...")
        
        retrieval_result = self.rag.retrieve(query)
        
        if retrieval_result["status"] != "success":
            return {
                "query": query,
                "status": "failed",
                "message": retrieval_result.get("message", "Kh√¥ng t√¨m th·∫•y th√¥ng tin"),
                "answer": None
            }
        
        if verbose:
            print(f"   Found: {retrieval_result['primary_chunk']['reference']}")
            if retrieval_result.get('escalations_applied', 0) > 0:
                print(f"   Escalations applied: {retrieval_result['escalations_applied']}")
            if retrieval_result['primary_chunk'].get('point_deduction'):
                print(f"   Point deduction: {retrieval_result['primary_chunk']['point_deduction']} ƒëi·ªÉm")

        # Step 2: Generate
        if verbose:
            print("\n[2/2] Generating answer...")
        
        answer = self.generate_answer(query, retrieval_result)
        
        if verbose:
            print(f"   Answer generated ({len(answer)} characters)")
        
        return {
            "query": query,
            "status": "success",
            "retrieval": retrieval_result,
            "answer": answer
        }
    
    def print_result(self, result: dict):
        """Pretty print the result"""
        
        print(f"\n{'='*80}")
        print("RESULT")
        print(f"{'='*80}")
        
        if result["status"] == "success":
            print(f"\nANSWER:\n{result['answer']}")
            
            print(f"\nSOURCE:")
            primary = result['retrieval']['primary_chunk']
            print(f"   Reference: {primary['reference']}")
            if primary.get('penalty'):
                print(f"   Penalty: {primary['penalty']['text']}")
            if primary.get('point_deduction'):
                print(f"   Point Deduction: {primary['point_deduction']} ƒëi·ªÉm")
            if primary.get('license_suspension'):
                print(f"   License Suspension: {primary['license_suspension']['text']}")
            print(f"   Tags: {', '.join(primary['tags'])}")
            print(f"   Escalation: {primary['is_escalation']} (Priority: {primary['priority']})")
        else:
            print(f"\n‚ùå {result['message']}")
        
        print(f"\n{'='*80}")


def test_with_model():
    """Test the complete system with various queries"""
    
    # Initialize system
    qa_system = TrafficLawQAWithPoints(
        data_path=r"D:\crawl_law\nd168_metadata_clean.json",
        model_name=r"D:\crawl_law\qwen3-0.6B-instruct-trafficlaws\checkpoint-13176"
    )
    
    # Test cases
    test_cases = [
        # {
        #     "query": "ƒëi xe m√°y kh√¥ng mang gi·∫•y ph√©p l√°i xe th√¨ sao?",
        #     "description": "Speeding 25km/h - should show 6-8M penalty + 6 points + 2-4 months suspension"
        # },
        # {
        #     "query": "l√°i √¥ t√¥ kh√¥ng b·∫≠t ƒë√®n th√¨ sao?",
        #     "description": "Right turn causing accident - escalation case"
        # },
        # {
        #     "query": "c·ªï v≈©, t·ª• t·∫≠p ƒëua xe m√°y b·ªã ph·∫°t sao?",
        #     "description": "Right turn causing accident - escalation case"
        # },
        {
            "query": "Xe √¥ t√¥ v∆∞·ª£t ƒë√®n ƒë·ªè v√† g√¢y tai n·∫°n b·ªã ph·∫°t bao nhi√™u v√† tr·ª´ m·∫•y ƒëi·ªÉm?",
            "description": "Red light violation with point deduction"
        },
        {
            "query": "Kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm khi ƒëi xe m√°y c√≥ b·ªã ph·∫°t kh√¥ng?",
            "description": "Motorcycle helmet violation"
        },
        {
            "query": "Xe √¥ t√¥ r·∫Ω ph·∫£i sai quy ƒë·ªãnh g√¢y tai n·∫°n th√¨ ph·∫°t nh∆∞ th·∫ø n√†o?",
            "description": "Motorcycle helmet violation"
        },
        # {
        #     "query": "ƒëi xe √¥ t√¥ l·∫°ng l√°ch ƒë√°nh v√µng g√¢y tai n·∫°n ph·∫°t ra sao?",
        #     "description": "Motorcycle helmet violation"
        # }
    ]
    
    print("\n" + "="*80)
    print("TESTING RAG WITH POINT DEDUCTION + QWEN 3")
    print("="*80)
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n\n{'#'*80}")
        print(f"TEST CASE {i}/{len(test_cases)}: {test_case['description']}")
        print(f"{'#'*80}")
        
        result = qa_system.ask(test_case["query"], verbose=True)
        qa_system.print_result(result)
        
        results.append(result)
    
    # Summary
    print(f"\n\n{'='*80}")
    print("SUMMARY")
    print(f"{'='*80}")
    
    success_count = sum(1 for r in results if r["status"] == "success")
    print(f"Successful: {success_count}/{len(results)}")
    print(f"Failed: {len(results) - success_count}/{len(results)}")
    
    # Save results
    output_file = r"D:\crawl_law\qa_test_results_with_points.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Results saved to: {output_file}")


if __name__ == "__main__":
    test_with_model()
