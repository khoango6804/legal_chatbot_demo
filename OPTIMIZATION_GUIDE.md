# ğŸš€ HÆ°á»›ng Dáº«n Tá»‘i Æ¯u HÃ³a Cho Server CPU

## ğŸ“Š ÄÃ¡nh GiÃ¡ Hiá»‡u Suáº¥t

Model **Qwen3 0.6B** (28 layers, 1024 hidden size) cÃ³ thá»ƒ cháº¡y trÃªn CPU server, nhÆ°ng cáº§n tá»‘i Æ°u hÃ³a Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t.

### Hiá»‡u Suáº¥t Dá»± Kiáº¿n:
- **CPU (khÃ´ng tá»‘i Æ°u)**: ~5-10 giÃ¢y/cÃ¢u tráº£ lá»i
- **CPU (Ä‘Ã£ tá»‘i Æ°u)**: ~2-5 giÃ¢y/cÃ¢u tráº£ lá»i
- **GPU**: ~1-2 giÃ¢y/cÃ¢u tráº£ lá»i

## âœ… CÃ¡c Tá»‘i Æ¯u ÄÃ£ Ãp Dá»¥ng

### 1. **Quantization (8-bit)**
- Giáº£m memory usage xuá»‘ng ~50%
- TÄƒng tá»‘c Ä‘á»™ inference ~30-40%
- CÃ i Ä‘áº·t: `pip install bitsandbytes`

### 2. **CPU Threading Optimization**
- Tá»± Ä‘á»™ng detect sá»‘ CPU cores
- Sá»­ dá»¥ng tá»‘i Ä‘a 8 threads
- Tá»‘i Æ°u inter-op vÃ  intra-op threads

### 3. **Model Compilation (torch.compile)**
- Compile model vá»›i PyTorch 2.0+
- TÄƒng tá»‘c Ä‘á»™ ~20-30%
- Chá»‰ Ã¡p dá»¥ng cho CPU mode

### 4. **Response Caching**
- Cache 100 cÃ¢u tráº£ lá»i gáº§n nháº¥t
- TrÃ¡nh tÃ­nh toÃ¡n láº¡i cho cÃ¢u há»i giá»‘ng nhau
- TÄƒng tá»‘c Ä‘á»™ pháº£n há»“i lÃªn ~100x cho cÃ¢u há»i Ä‘Ã£ cache

### 5. **Generation Parameters Optimization**
- **CPU**: Greedy decoding (nhanh hÆ¡n sampling)
- **CPU**: Giáº£m max_new_tokens tá»« 512 â†’ 256
- **GPU**: Giá»¯ nguyÃªn settings tá»‘i Æ°u

### 6. **Memory Optimization**
- `low_cpu_mem_usage=True`
- Sá»­ dá»¥ng float32 cho CPU (tá»‘t hÆ¡n float16)
- Efficient memory management

## ğŸ”§ CÃ i Äáº·t Tá»‘i Æ¯u

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies cÆ¡ báº£n
```bash
pip install -r requirements.txt
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t quantization (TÃ¹y chá»n - Khuyáº¿n nghá»‹)
```bash
# Windows (cáº§n Visual Studio Build Tools)
pip install bitsandbytes

# Linux/Mac
pip install bitsandbytes
```

### BÆ°á»›c 3: Kiá»ƒm tra PyTorch version
```bash
python -c "import torch; print(torch.__version__)"
# Cáº§n PyTorch 2.0+ Ä‘á»ƒ sá»­ dá»¥ng torch.compile
```

## ğŸ“ˆ Benchmarking

### Test hiá»‡u suáº¥t:
```bash
python speed_test.py
```

### So sÃ¡nh trÆ°á»›c/sau tá»‘i Æ°u:
```bash
python compare_speed.py
```

## ğŸ¯ Khuyáº¿n Nghá»‹ Cho Production

### Server Requirements:
- **CPU**: 4+ cores (khuyáº¿n nghá»‹ 8+ cores)
- **RAM**: 8GB+ (khuyáº¿n nghá»‹ 16GB+)
- **OS**: Linux (tá»‘t hÆ¡n Windows cho CPU inference)

### Tá»‘i Æ¯u ThÃªm:

1. **Sá»­ dá»¥ng ONNX Runtime** (TÃ¹y chá»n - nÃ¢ng cao):
   ```bash
   pip install onnxruntime
   ```
   - Chuyá»ƒn Ä‘á»•i model sang ONNX format
   - TÄƒng tá»‘c Ä‘á»™ thÃªm ~20-30%

2. **Sá»­ dá»¥ng GGML/llama.cpp** (TÃ¹y chá»n - nÃ¢ng cao):
   - Chuyá»ƒn Ä‘á»•i model sang GGML format
   - Tá»‘i Æ°u cá»±c ká»³ cho CPU
   - Cáº§n compile tá»« source

3. **Load Balancing**:
   - Sá»­ dá»¥ng multiple workers vá»›i uvicorn
   ```bash
   uvicorn app:app --workers 2 --host 0.0.0.0 --port 8000
   ```

4. **Caching Layer**:
   - Sá»­ dá»¥ng Redis cho distributed caching
   - TÄƒng kháº£ nÄƒng scale

5. **CDN/Reverse Proxy**:
   - Sá»­ dá»¥ng Nginx Ä‘á»ƒ cache static files
   - Giáº£m táº£i cho application server

## âš™ï¸ Cáº¥u HÃ¬nh NÃ¢ng Cao

### Environment Variables:
```bash
# Sá»‘ threads cho CPU
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

# Tá»‘i Æ°u memory
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### Uvicorn Configuration:
```python
# app.py
if __name__ == "__main__":
    import multiprocessing
    workers = min(multiprocessing.cpu_count(), 4)
    uvicorn.run(
        "app:app", 
        host="0.0.0.0", 
        port=8000, 
        workers=workers,  # Multiple workers
        log_level="info"
    )
```

## ğŸ“Š Monitoring

### Theo dÃµi hiá»‡u suáº¥t:
- Tá»‘c Ä‘á»™ token/s Ä‘Æ°á»£c hiá»ƒn thá»‹ trong response
- Log CPU usage vÃ  memory
- Monitor response time

### Tools:
- `htop` hoáº·c `top` Ä‘á»ƒ monitor CPU/RAM
- `nvidia-smi` náº¿u cÃ³ GPU
- Application logs

## ğŸ› Troubleshooting

### Lá»—i: "bitsandbytes not available"
- **Giáº£i phÃ¡p**: CÃ i Ä‘áº·t `bitsandbytes` hoáº·c bá» qua (code sáº½ tá»± Ä‘á»™ng fallback)

### Lá»—i: "torch.compile not available"
- **Giáº£i phÃ¡p**: NÃ¢ng cáº¥p PyTorch lÃªn 2.0+ hoáº·c bá» qua (code sáº½ tá»± Ä‘á»™ng fallback)

### Cháº­m trÃªn CPU:
1. Kiá»ƒm tra sá»‘ CPU cores: `python -c "import multiprocessing; print(multiprocessing.cpu_count())"`
2. TÄƒng sá»‘ threads trong code náº¿u cáº§n
3. Sá»­ dá»¥ng quantization
4. Giáº£m `max_new_tokens` xuá»‘ng 128-192

### Memory issues:
1. Giáº£m `MAX_CACHE_SIZE` trong `inference.py`
2. Sá»­ dá»¥ng quantization
3. TÄƒng swap memory
4. Giáº£m sá»‘ workers

## ğŸ“ Notes

- Model 0.6B khÃ¡ nhá», phÃ¹ há»£p cho CPU inference
- Vá»›i model lá»›n hÆ¡n (>1B), nÃªn cÃ¢n nháº¯c GPU hoáº·c quantization máº¡nh hÆ¡n
- Response caching ráº¥t hiá»‡u quáº£ cho production (nhiá»u user há»i cÃ¢u tÆ°Æ¡ng tá»±)

## ğŸ‰ Káº¿t Luáº­n

Vá»›i cÃ¡c tá»‘i Æ°u Ä‘Ã£ Ã¡p dá»¥ng, model Qwen3 0.6B cÃ³ thá»ƒ cháº¡y tá»‘t trÃªn server CPU vá»›i:
- âœ… Tá»‘c Ä‘á»™: 2-5 giÃ¢y/cÃ¢u tráº£ lá»i
- âœ… Memory: ~2-4GB RAM
- âœ… CÃ³ thá»ƒ handle nhiá»u requests Ä‘á»“ng thá»i
- âœ… Response caching giÃºp tÄƒng tá»‘c Ä‘á»™ Ä‘Ã¡ng ká»ƒ

**Khuyáº¿n nghá»‹**: Sá»­ dá»¥ng quantization vÃ  response caching cho production!

