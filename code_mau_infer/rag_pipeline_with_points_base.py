#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced RAG Pipeline for Traffic Law with License Point Deduction
Extends rag_pipeline_qwen3.py with point deduction tracking
"""

import json
import re
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict

# Behavior keyword mapping
BEHAVIOR_KEYWORDS = {
    "vuot_den": ["v∆∞·ª£t ƒë√®n", "v∆∞·ª£t ƒë√®n ƒë·ªè", "kh√¥ng ch·∫•p h√†nh hi·ªáu l·ªánh c·ªßa ƒë√®n", "v∆∞·ª£t ƒë√®n t√≠n hi·ªáu"],
    "re_phai": ["r·∫Ω ph·∫£i", "r·∫Ω tr√°i", "quay ƒë·∫ßu xe", "chuy·ªÉn h∆∞·ªõng"],
    "can_vach": ["c√°n v·∫°ch", "v·∫°ch ph√¢n l√†n", "v·∫°ch k·∫ª ƒë∆∞·ªùng", "c√°n l√™n v·∫°ch"],
    "chuyen_lan": ["chuy·ªÉn l√†n", "ƒë·ªïi l√†n", "sang l√†n"],
    "luot_trai_pha_cam": ["l·∫•n l√†n", "ƒëi ng∆∞·ª£c chi·ªÅu", "v∆∞·ª£t ·∫©u", "ƒëi kh√¥ng ƒë√∫ng l√†n"],
    "qua_toc_do": ["qu√° t·ªëc ƒë·ªô", "v∆∞·ª£t t·ªëc ƒë·ªô", "ch·∫°y qu√° t·ªëc ƒë·ªô", "t·ªëc ƒë·ªô"],
    "toc_do": ["qu√° t·ªëc ƒë·ªô", "v∆∞·ª£t t·ªëc ƒë·ªô", "ch·∫°y qu√° t·ªëc ƒë·ªô", "t·ªëc ƒë·ªô"],  # Alias for ND168 compatibility
    "khong_doi_mu": ["kh√¥ng ƒë·ªôi m≈©", "m≈© b·∫£o hi·ªÉm", "kh√¥ng ƒë·ªôi n√≥n"],
    "dung_do_sai": ["d·ª´ng xe", "ƒë·ªó xe", "ƒë·∫≠u xe sai quy ƒë·ªãnh"],
    "chay_khu_cam": ["khu v·ª±c c·∫•m", "n∆°i c·∫•m d·ª´ng", "n∆°i c·∫•m ƒë·ªó"],
    "dien_thoai": ["ƒëi·ªán tho·∫°i", "nghe ƒëi·ªán tho·∫°i", "d√πng ƒëi·ªán tho·∫°i", "s·ª≠ d·ª•ng ƒëi·ªán tho·∫°i", "thi·∫øt b·ªã ƒëi·ªán t·ª≠"],
    "khong_bat_den": ["kh√¥ng b·∫≠t ƒë√®n", "kh√¥ng b·∫≠t xi-nhan", "kh√¥ng c√≥ ƒë√®n", "kh√¥ng s·ª≠ d·ª•ng ƒë√®n"],
    "cho_qua_nguoi": ["ch·ªü qu√° ng∆∞·ªùi", "qu√° s·ªë ng∆∞·ªùi", "v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng"],
    "khong_bang_lai": ["kh√¥ng c√≥ b·∫±ng l√°i", "kh√¥ng gi·∫•y ph√©p", "ch∆∞a c√≥ b·∫±ng"],
    "gay_tai_nan": ["g√¢y tai n·∫°n", "tai n·∫°n giao th√¥ng", "va ch·∫°m g√¢y th∆∞∆°ng t√≠ch"],
    "uong_ruou_bia": ["n·ªìng ƒë·ªô c·ªìn", "u·ªëng r∆∞·ª£u", "u·ªëng bia", "say r∆∞·ª£u", "c√≥ c·ªìn"],
    "lang_lach": ["l·∫°ng l√°ch", "ƒë√°nh v√µng", "drift", "bi·ªÉu di·ªÖn"],
    "boc_dau": ["b·ªëc ƒë·∫ßu", "ch·∫°y b·∫±ng m·ªôt b√°nh", "n√¢ng b√°nh tr∆∞·ªõc", "wheelie", "ch·∫°y m·ªôt b√°nh"],
    "khong_nhuong_duong": ["kh√¥ng nh∆∞·ªùng ƒë∆∞·ªùng", "c·∫£n tr·ªü xe ∆∞u ti√™n", "kh√¥ng gi·∫£m t·ªëc"],
    "vuot_xe_sai": ["v∆∞·ª£t xe", "v∆∞·ª£t b√™n ph·∫£i", "v∆∞·ª£t kh√¥ng ƒë√∫ng"],
    "tram_cam": ["c·∫ßm m√°y", "kh√¥ng t·∫Øt m√°y", "ƒë·ªÉ m√°y n·ªï", "c√≤i", "r√∫ ga", "n·∫πt p√¥"],
    "cho_hang": ["ch·ªü h√†ng", "qu√° t·∫£i", "ch·ªü qu√° kh·ªï", "v∆∞·ª£t qu√° t·∫£i tr·ªçng"],
    "dieu_khien_nguy_hiem": ["bu√¥ng c·∫£ hai tay", "d√πng ch√¢n ƒëi·ªÅu khi·ªÉn", "ng·ªìi v·ªÅ m·ªôt b√™n", "n·∫±m tr√™n y√™n", 
                             "thay ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn", "quay ng∆∞·ªùi v·ªÅ ph√≠a sau", "b·ªãt m·∫Øt ƒëi·ªÅu khi·ªÉn",
                             "bu√¥ng tay", "kh√¥ng c·∫ßm tay l√°i", "ƒëi·ªÅu khi·ªÉn b·∫±ng ch√¢n"],
    "khong_bien_so": ["kh√¥ng g·∫Øn bi·ªÉn s·ªë", "bi·ªÉn s·ªë xe", "bi·ªÉn ki·ªÉm so√°t", "kh√¥ng c√≥ bi·ªÉn s·ªë", "g·∫Øn bi·ªÉn s·ªë kh√¥ng ƒë√∫ng"],
    "khong_giay_to": ["kh√¥ng mang gi·∫•y", "gi·∫•y ch·ª©ng nh·∫≠n", "ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω", "kh√¥ng c√≥ gi·∫•y t·ªù"],
    "khong_mang_bang_lai": ["kh√¥ng mang theo gi·∫•y ph√©p l√°i xe", "kh√¥ng mang theo b·∫±ng l√°i", "kh√¥ng mang gi·∫•y ph√©p", "kh√¥ng mang b·∫±ng l√°i"],
    "khong_bang_lai": ["kh√¥ng c√≥ gi·∫•y ph√©p l√°i xe", "kh√¥ng c√≥ b·∫±ng l√°i"],
    "keo_xe": ["k√©o xe", "k√©o r∆° mo√≥c", "k√©o theo xe kh√°c"],
    "dan_hang_ngang": ["d√†n h√†ng ngang", "ch·∫°y d√†n h√†ng", "ƒëi song song"],
    "chay_trong_ham": ["ch·∫°y trong h·∫ßm", "h·∫ßm ƒë∆∞·ªùng b·ªô"],
    "khong_thu_phi": ["kh√¥ng thu ph√≠", "kh√¥ng d·ª´ng thu ph√≠", "thu ph√≠ ƒëi·ªán t·ª≠"],
    "van_tai": ["v·∫≠n t·∫£i", "kinh doanh v·∫≠n t·∫£i", "ho·∫°t ƒë·ªông v·∫≠n t·∫£i"],
    "tai_pham": ["t√°i ph·∫°m", "vi ph·∫°m l·∫ßn 2", "vi ph·∫°m l·∫°i"],
    "dua_xe": ["ƒëua xe", "ƒëua xe tr√°i ph√©p", "ch·∫°y ƒëua", "ƒëua t·ªëc ƒë·ªô"],
    "co_vu_dua_xe": ["c·ªï v≈© ƒëua xe", "t·ª• t·∫≠p ƒëua xe", "c·ªï v≈©", "t·ª• t·∫≠p ƒë·ªÉ c·ªï v≈©", "gi√∫p s·ª©c ƒëua xe", "x√∫i gi·ª•c ƒëua xe"]
}

# Escalation indicators
ESCALATION_INDICATORS = {
    "gay_tai_nan": ["g√¢y tai n·∫°n", "l√†m ch·∫øt ng∆∞·ªùi", "g√¢y th∆∞∆°ng t√≠ch", "tai n·∫°n giao th√¥ng"],
    "tron_chay": ["b·ªè ch·∫°y", "t·∫©u tho√°t", "r·ªùi kh·ªèi hi·ªán tr∆∞·ªùng"],
    "vi_pham_nghiem_trong": ["vi ph·∫°m nghi√™m tr·ªçng", "t√°i ph·∫°m", "vi ph·∫°m nhi·ªÅu l·∫ßn"],
    "khong_chap_hanh": ["kh√¥ng ch·∫•p h√†nh", "c·∫£n tr·ªü", "ch·ªëng ƒë·ªëi"]
}


@dataclass
class ChunkMetadata:
    """Metadata for each chunk with point deduction"""
    article: int
    khoan: Optional[int]  # Changed to Optional for law chunks
    diem: Optional[str]
    tags: Set[str]
    is_escalation: bool
    escalation_refs: Set[Tuple[int, int, Optional[str]]]
    priority: int
    content: str
    penalty_min: Optional[int] = None
    penalty_max: Optional[int] = None
    point_deduction: Optional[int] = None
    license_suspension_months: Optional[Tuple[int, int]] = None  # (min, max) months
    references: Optional[List[dict]] = None  # Raw references from JSON
    source: Optional[str] = None  # "nd168", "luat_duong_bo", or "luat_ttatgtdb"
    record_type: Optional[str] = None  # "penalty", "concept", or "rule"


class TrafficLawRAGWithPoints:
    """Enhanced RAG Pipeline with License Point Deduction"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.chunks: List[ChunkMetadata] = []
        self.behavior_index: Dict[str, List[int]] = defaultdict(list)
        self.escalation_chunks: List[int] = []
        self.reference_index: Dict[Tuple[int, int, Optional[str]], List[int]] = defaultdict(list)
        
        self._load_and_process_data()
    
    def _extract_tags(self, text: str) -> Set[str]:
        """Extract behavior tags from text"""
        text_lower = text.lower()
        tags = set()
        
        for tag, keywords in BEHAVIOR_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text_lower:
                    tags.add(tag)
                    break
        
        return tags
    
    def _detect_vehicle_type(self, query: str) -> int:
        """Detect vehicle type from query (returns article number)"""
        query_lower = query.lower()
        
        # Check for xe m√¥ t√¥ / xe m√°y keywords first (more specific)
        moto_keywords = ['xe m√¥ t√¥', 'xe m√°y', 'm√¥ t√¥', 'xe g·∫Øn m√°y']
        if any(keyword in query_lower for keyword in moto_keywords):
            return 7  # ƒêi·ªÅu 7 for motorcycles
        
        # Check for xe √¥ t√¥ keywords
        car_keywords = ['xe √¥ t√¥', '√¥ t√¥', 'xe h∆°i']
        if any(keyword in query_lower for keyword in car_keywords):
            return 6  # ƒêi·ªÅu 6 for cars
        
        # Default to car if no specific vehicle mentioned
        return 6
    
    def _extract_speed_range(self, query: str) -> Optional[int]:
        """Extract speed violation amount from query (in km/h)"""
        # Patterns for speed extraction
        patterns = [
            r'qu√° t·ªëc ƒë·ªô\s+(\d+)\s*km',
            r'v∆∞·ª£t t·ªëc ƒë·ªô\s+(\d+)\s*km',
            r'ch·∫°y qu√° t·ªëc\s*(?:ƒë·ªô)?\s+(\d+)\s*km',
            r't·ªëc ƒë·ªô\s+(\d+)\s*km'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query.lower())
            if match:
                return int(match.group(1))
        
        return None
    
    def _get_speed_violation_khoan(self, article: int, speed_kmh: int, has_accident: bool) -> Optional[int]:
        """
        Determine the appropriate kho·∫£n based on speed violation amount
        
        Article 6 (√î t√¥):
        - kho·∫£n 4 ƒëi·ªÉm i: 5-10 km/h (800K-1M)
        - kho·∫£n 5: 10-20 km/h (4M-6M) 
        - kho·∫£n 6 ƒëi·ªÉm a: 20-35 km/h (6M-8M) ‚Üê 25km/h falls here
        - kho·∫£n 7 ƒëi·ªÉm a: > 35 km/h (12M-14M)
        - kho·∫£n 10: any speed + accident (20M-22M)
        
        Article 7 (M√¥ t√¥):
        - kho·∫£n 4 ƒëi·ªÉm a: 10-20 km/h (800K-1M)
        - kho·∫£n 8 ƒëi·ªÉm a: > 20 km/h (6M-8M) ‚Üê 25km/h falls here
        - kho·∫£n 10: any speed + accident (10M-14M)
        """
        if article == 6:  # Xe √¥ t√¥
            if has_accident:
                return 10  # Any speed + accident
            elif speed_kmh <= 10:
                return 4  # 5-10 km/h
            elif speed_kmh <= 20:
                return 5  # 10-20 km/h
            elif speed_kmh <= 35:
                return 6  # 20-35 km/h (25 falls here)
            else:
                return 7  # > 35 km/h
        
        elif article == 7:  # Xe m√¥ t√¥
            if has_accident:
                return 10  # Any speed + accident
            elif speed_kmh <= 20:
                return 4  # 10-20 km/h
            else:
                return 8  # > 20 km/h (25 falls here)
        
        return None
    
    def _is_escalation_chunk(self, text: str) -> bool:
        """Check if chunk describes escalation rules"""
        text_lower = text.lower()
        escalation_patterns = [
            r'n√¢ng m·ª©c',
            r'tƒÉng m·ª©c',
            r'ph·∫°t cao h∆°n',
            r'cao h∆°n m·ª©c',
            r'quy ƒë·ªãnh t·∫°i ƒëi·ªÅu \d+ kho·∫£n \d+',
            r'c√°c h√†nh vi.*ƒëi·ªÉm [a-z]',
            r'm·ªôt trong c√°c h√†nh vi'
        ]
        
        return any(re.search(pattern, text_lower) for pattern in escalation_patterns)
    
    def _extract_escalation_refs(self, text: str) -> Set[Tuple[int, int, Optional[str]]]:
        """Extract references from escalation chunks"""
        refs = set()
        
        # Pattern: ƒêi·ªÅu X kho·∫£n Y ƒëi·ªÉm Z
        pattern1 = re.compile(r'ƒêi·ªÅu\s+(\d+)\s+kho·∫£n\s+(\d+)\s+ƒëi·ªÉm\s+([a-z]|ƒë)', re.IGNORECASE)
        for match in pattern1.finditer(text):
            refs.add((int(match.group(1)), int(match.group(2)), match.group(3).lower()))
        
        # Pattern: ƒêi·ªÅu X kho·∫£n Y
        pattern2 = re.compile(r'ƒêi·ªÅu\s+(\d+)\s+kho·∫£n\s+(\d+)', re.IGNORECASE)
        for match in pattern2.finditer(text):
            if not any(ref[0] == int(match.group(1)) and ref[1] == int(match.group(2)) for ref in refs):
                refs.add((int(match.group(1)), int(match.group(2)), None))
        
        # Pattern: kho·∫£n X ƒëi·ªÉm Y (same article)
        pattern3 = re.compile(r'kho·∫£n\s+(\d+)\s+ƒëi·ªÉm\s+([a-z]|ƒë)', re.IGNORECASE)
        for match in pattern3.finditer(text):
            refs.add((None, int(match.group(1)), match.group(2).lower()))
        
        # Pattern: ƒëi·ªÉm a, b, c
        pattern4 = re.compile(r'ƒëi·ªÉm\s+([a-z]|ƒë)(?:\s*,\s*([a-z]|ƒë))*', re.IGNORECASE)
        for match in pattern4.finditer(text):
            letters = re.findall(r'\b([a-z]|ƒë)\b', match.group(0).lower())
            for letter in letters:
                refs.add((None, None, letter))
        
        return refs
    
    def _extract_penalty_amounts(self, text: str) -> Tuple[Optional[int], Optional[int]]:
        """Extract penalty amounts from text (in thousands VND)"""
        patterns = [
            r'(\d+)\.(\d+)\.000\s*ƒë·ªìng',
            r'(\d+)\.(\d+)\s*tri·ªáu',
            r'(\d+)\s*tri·ªáu',
            r'(\d+)\s*ƒë·∫øn\s*(\d+)\s*tri·ªáu'
        ]
        
        amounts = []
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if 'ƒë·∫øn' in match.group(0):
                    min_val = int(match.group(1)) * 1000
                    max_val = int(match.group(2)) * 1000
                    return min_val, max_val
                elif '.' in match.group(0) and 'tri·ªáu' not in match.group(0):
                    val = int(match.group(1) + match.group(2))
                    amounts.append(val)
                else:
                    val = int(match.group(1)) * 1000
                    amounts.append(val)
        
        if amounts:
            return min(amounts), max(amounts)
        
        return None, None
    
    def _extract_point_deduction(self, article: int, khoan: int, diem: Optional[str]) -> Optional[int]:
        """
        Extract license point deduction based on Article 6 kho·∫£n 16 and Article 7 kho·∫£n 13
        
        Article 6 (√î t√¥) - kho·∫£n 16:
        - 2 ƒëi·ªÉm: kho·∫£n 3 (h,i); kho·∫£n 4 (a,b,c,d,ƒë,g); kho·∫£n 5 (a,b,c,d,ƒë,e,g,i,k,n,o)
        - 4 ƒëi·ªÉm: kho·∫£n 5 (h); kho·∫£n 6; kho·∫£n 7 (b); kho·∫£n 9 (b,c,d)
        - 6 ƒëi·ªÉm: kho·∫£n 5 (p); kho·∫£n 7 (a,c); kho·∫£n 8
        - 10 ƒëi·ªÉm: kho·∫£n 9 (a); kho·∫£n 10; kho·∫£n 11 (ƒë)
        
        Article 7 (M√¥ t√¥) - kho·∫£n 13:
        - 2 ƒëi·ªÉm: kho·∫£n 3 (b); kho·∫£n 5; kho·∫£n 6 (b,c,d)
        - 4 ƒëi·ªÉm: kho·∫£n 4 (ƒë); kho·∫£n 6 (a); kho·∫£n 7 (c,d,ƒë); kho·∫£n 8 (a)  ‚Üê 25km/h here!
        - 6 ƒëi·ªÉm: kho·∫£n 7 (b); kho·∫£n 9 (c)
        - 10 ƒëi·ªÉm: kho·∫£n 8 (b); kho·∫£n 10
        """
        if article == 6:  # Xe √¥ t√¥
            # 10 points
            if (khoan == 9 and diem == 'a') or khoan == 10 or (khoan == 11 and diem == 'ƒë'):
                return 10
            # 6 points
            elif (khoan == 5 and diem == 'p') or (khoan == 7 and diem in ['a', 'c']) or khoan == 8:
                return 6
            # 4 points
            elif (khoan == 5 and diem == 'h') or khoan == 6 or (khoan == 7 and diem == 'b') or (khoan == 9 and diem in ['b', 'c', 'd']):
                return 4
            # 2 points
            elif (khoan == 3 and diem in ['h', 'i']) or \
                 (khoan == 4 and diem in ['a', 'b', 'c', 'd', 'ƒë', 'g']) or \
                 (khoan == 5 and diem in ['a', 'b', 'c', 'd', 'ƒë', 'e', 'g', 'i', 'k', 'n', 'o']):
                return 2
        
        elif article == 7:  # Xe m√¥ t√¥
            # 10 points - Note: kho·∫£n 8 ƒëi·ªÉm b (alcohol), not kho·∫£n 8 ƒëi·ªÉm a (speed)
            if (khoan == 8 and diem == 'b') or khoan == 10:
                return 10
            # 6 points
            elif (khoan == 7 and diem == 'b') or (khoan == 9 and diem == 'c'):
                return 6
            # 4 points - kho·∫£n 8 ƒëi·ªÉm a (speed >20km/h) is here!
            elif (khoan == 4 and diem == 'ƒë') or (khoan == 6 and diem == 'a') or \
                 (khoan == 7 and diem in ['c', 'd', 'ƒë']) or (khoan == 8 and diem == 'a'):
                return 4
            # 2 points
            elif (khoan == 3 and diem == 'b') or khoan == 5 or (khoan == 6 and diem in ['b', 'c', 'd']):
                return 2
        
        return None
    
    def _extract_license_suspension(self, article: int, khoan: int) -> Optional[Tuple[int, int]]:
        """
        Extract license suspension duration (in months) based on article and khoan
        Based on Ngh·ªã ƒë·ªãnh 168 - each kho·∫£n specifies suspension duration
        """
        if article == 6:  # Xe √¥ t√¥
            if khoan == 5:
                return (1, 3)  # 1-3 months
            elif khoan == 6:
                return (2, 4)  # 2-4 months  
            elif khoan == 7:
                return (2, 4)  # 2-4 months
            elif khoan == 8:
                return (3, 5)  # 3-5 months
            elif khoan == 9:
                return (4, 6)  # 4-6 months
            elif khoan == 10:
                return (5, 7)  # 5-7 months
            elif khoan == 11:
                return (6, 8)  # 6-8 months
        
        elif article == 7:  # Xe m√¥ t√¥
            if khoan == 4:
                return (1, 3)
            elif khoan == 5:
                return (1, 3)
            elif khoan == 6:
                return (2, 4)
            elif khoan == 7:
                return (2, 4)
            elif khoan == 8:
                return (3, 5)
            elif khoan == 9:
                return (4, 6)
            elif khoan == 10:
                return (5, 7)
            elif khoan == 11:
                return (22, 24)  # Wheelie/dangerous stunts - 22-24 months
        
        return None
    
    def _load_and_process_data(self):
        """Load and process the metadata file into chunks with point deduction"""
        print("Loading data from", self.data_path)
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            if self.data_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f]
            else:
                data = json.load(f)
        
        print(f"   Found {len(data)} records")
        
        for idx, record in enumerate(data):
            content = record.get('content') or record.get('text') or record.get('diem_text', '')
            article = record.get('article_num') or record.get('article')
            khoan = record.get('khoan_num') or record.get('khoan') or record.get('clause')
            diem = record.get('diem_letter') or record.get('diem') or record.get('point')
            
            # For law records (not penalties), khoan can be None
            if not content or not article:
                continue
            
            # USE tags from JSON if available (more accurate), otherwise extract from content
            tags = set(record.get('tags', []))
            if not tags:
                tags = self._extract_tags(content)
            
            # USE is_escalation from JSON if available, otherwise detect
            is_escalation = record.get('is_escalation', False)
            if is_escalation is None:  # If not specified in JSON
                is_escalation = self._is_escalation_chunk(content)
            
            escalation_refs = set()
            if is_escalation:
                escalation_refs = self._extract_escalation_refs(content)
                filled_refs = set()
                for ref in escalation_refs:
                    if ref[0] is None:
                        filled_refs.add((article, ref[1] or khoan, ref[2]))
                    else:
                        filled_refs.add(ref)
                escalation_refs = filled_refs
            
            # USE penalty from JSON if available, otherwise extract
            penalty_min = record.get('penalty_min')
            penalty_max = record.get('penalty_max')
            if penalty_min is None or penalty_max is None:
                penalty_min, penalty_max = self._extract_penalty_amounts(content)
            
            point_deduction = self._extract_point_deduction(article, khoan, diem)
            license_suspension = self._extract_license_suspension(article, khoan)
            
            # Get references from JSON if available
            references = record.get('references', []) if record.get('references') else []
            
            priority = 50
            if is_escalation:
                if any(keyword in content.lower() for keyword in ESCALATION_INDICATORS["gay_tai_nan"]):
                    priority = 100
                elif escalation_refs:
                    priority = 90
                else:
                    priority = 80
            
            # Get source and record_type from record if available
            source = record.get('source', 'nd168')  # Default to nd168
            record_type = record.get('record_type', 'penalty')  # Default to penalty
            
            chunk = ChunkMetadata(
                article=article,
                khoan=khoan,
                diem=diem,
                tags=tags,
                is_escalation=is_escalation,
                escalation_refs=escalation_refs,
                priority=priority,
                content=content,
                penalty_min=penalty_min,
                penalty_max=penalty_max,
                point_deduction=point_deduction,
                license_suspension_months=license_suspension,
                references=references,
                source=source,
                record_type=record_type
            )
            
            chunk_idx = len(self.chunks)
            self.chunks.append(chunk)
            
            for tag in tags:
                self.behavior_index[tag].append(chunk_idx)
            
            if is_escalation:
                self.escalation_chunks.append(chunk_idx)
            
            ref = (article, khoan, diem)
            self.reference_index[ref].append(chunk_idx)
        
        print(f"Processed {len(self.chunks)} chunks")
        print(f"   Behavior tags: {len(self.behavior_index)}")
        print(f"   Escalation chunks: {len(self.escalation_chunks)}")
        print(f"   Chunks with point deduction: {sum(1 for c in self.chunks if c.point_deduction)}")
    
    def retrieve(self, query: str) -> dict:
        """Retrieve relevant chunks with full details including point deduction"""
        
        query_tags = self._extract_tags(query)
        query_lower = query.lower()
        has_tai_nan = any(keyword in query_lower for keyword in ESCALATION_INDICATORS["gay_tai_nan"])
        
        # SPECIAL CASE: For individuals, "kh√¥ng mang" (not carrying) is legally same as "kh√¥ng c√≥" (not having)
        # Only business transport has special lower penalty for "kh√¥ng mang"
        if 'khong_mang_bang_lai' in query_tags and 'van_tai' not in query_tags:
            query_tags.add('khong_bang_lai')  # Also match "kh√¥ng c√≥" provisions
        
        # Detect subject type (individual vs organization)
        is_organization = any(keyword in query_lower for keyword in ['t·ªï ch·ª©c', 'doanh nghi·ªáp', 'c√¥ng ty', 'c∆° s·ªü', 'ƒë∆°n v·ªã'])
        
        # Detect vehicle type and speed violation
        vehicle_article = self._detect_vehicle_type(query)
        speed_kmh = self._extract_speed_range(query)
        
        # Detect engine size (for motorcycles)
        engine_size_tag = None
        if re.search(r'(\d+)\s*cc', query_lower, re.IGNORECASE):
            cc_match = re.search(r'(\d+)\s*cc', query_lower, re.IGNORECASE)
            cc_value = int(cc_match.group(1))
            if cc_value <= 125:
                engine_size_tag = 'engine_125cc_or_less'
                # Don't add to query_tags - only use for filtering, not matching
            else:
                engine_size_tag = 'engine_over_125cc'
                # Don't add to query_tags - only use for filtering, not matching
        
        print(f"\nQuery: {query}")
        print(f"   Detected tags: {query_tags}")
        print(f"   Subject: {'T·ªï ch·ª©c' if is_organization else 'C√° nh√¢n'}")
        print(f"   Vehicle type: {'Xe √¥ t√¥ (ƒêi·ªÅu 6)' if vehicle_article == 6 else 'Xe m√¥ t√¥ (ƒêi·ªÅu 7)'}")
        print(f"   Has tai n·∫°n: {has_tai_nan}")
        if speed_kmh:
            print(f"   Speed violation: {speed_kmh} km/h")
        if engine_size_tag:
            print(f"   Engine size: {engine_size_tag}")
        
        if not query_tags:
            return {
                "status": "no_tags",
                "message": "Kh√¥ng ph√°t hi·ªán h√†nh vi c·ª• th·ªÉ trong c√¢u h·ªèi"
            }
        
        # If speed violation detected, find the appropriate khoan
        target_khoan = None
        target_article = None
        if speed_kmh and "qua_toc_do" in query_tags:
            target_article = vehicle_article
            target_khoan = self._get_speed_violation_khoan(vehicle_article, speed_kmh, has_tai_nan)
            if target_khoan:
                print(f"   Target: ƒêi·ªÅu {target_article} kho·∫£n {target_khoan} for {speed_kmh}km/h")
        
        behavior_chunk_indices = set()
        for tag in query_tags:
            if tag in self.behavior_index:
                behavior_chunk_indices.update(self.behavior_index[tag])
        
        if not behavior_chunk_indices:
            return {
                "status": "no_chunks",
                "message": "Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t li√™n quan"
            }
        
        # Filter for ND168 chunks only (exclude law chunks)
        behavior_chunks = [self.chunks[idx] for idx in behavior_chunk_indices 
                          if not hasattr(self.chunks[idx], 'source') or self.chunks[idx].source == 'nd168']
        print(f"   Found {len(behavior_chunks)} behavior chunks")
        
        # DEBUG: Check if ND168_art18_k2_d is in behavior_chunks
        k2d_in_behaviors = any(c.article == 18 and c.khoan == 2 and c.diem == 'd' for c in behavior_chunks)
        if k2d_in_behaviors:
            print(f"   DEBUG: ND168_art18_k2_d IS in behavior_chunks")
        else:
            print(f"   DEBUG: ND168_art18_k2_d NOT in behavior_chunks")
        
        # FILTER BY SUBJECT TYPE: Default to individual (ƒêi·ªÅu 6-21), unless "t·ªï ch·ª©c" mentioned
        if not is_organization:
            # Individual violations: ƒêi·ªÅu 6-21 (traffic violations by individuals)
            individual_chunks = [c for c in behavior_chunks if 6 <= c.article <= 21]
            if individual_chunks:
                behavior_chunks = individual_chunks
                print(f"   Filtered to {len(behavior_chunks)} individual chunks (ƒêi·ªÅu 6-21)")
        else:
            # Organization violations: ƒêi·ªÅu 30+ (business/organization violations)
            org_chunks = [c for c in behavior_chunks if c.article >= 30]
            if org_chunks:
                behavior_chunks = org_chunks
                print(f"   Filtered to {len(behavior_chunks)} organization chunks (ƒêi·ªÅu 30+)")
        
        # FILTER BY VEHICLE TYPE: If vehicle type detected, prioritize matching article
        # This prevents xe √¥ t√¥ queries from matching ƒêi·ªÅu 7 (m√¥ t√¥) rules
        vehicle_filtered_chunks = [c for c in behavior_chunks if c.article == vehicle_article]
        if vehicle_filtered_chunks:
            behavior_chunks = vehicle_filtered_chunks
            print(f"   Filtered to {len(behavior_chunks)} chunks for vehicle article {vehicle_article}")
            # Debug: show what's in behavior_chunks
            for c in behavior_chunks:
                print(f"      - ƒêi·ªÅu {c.article} kho·∫£n {c.khoan} ƒëi·ªÉm {c.diem}: is_escalation={c.is_escalation}, penalty={c.penalty_min}-{c.penalty_max}")
        
        # If we have a target khoan for speed, further filter by khoan
        if target_khoan and target_article:
            speed_filtered_chunks = [
                c for c in behavior_chunks 
                if c.article == target_article and c.khoan == target_khoan
            ]
            if speed_filtered_chunks:
                behavior_chunks = speed_filtered_chunks
                print(f"   Filtered to {len(behavior_chunks)} chunks matching ƒêi·ªÅu {target_article} kho·∫£n {target_khoan}")
        
        behavior_refs = set()
        for chunk in behavior_chunks:
            behavior_refs.add((chunk.article, chunk.khoan, chunk.diem))
        
        # Collect ALL matching chunks (both base behaviors and escalations)
        matched_chunks = []
        
        # FIRST: Add base behavior chunks (non-escalation) that match tags
        for chunk in behavior_chunks:
            if not chunk.is_escalation and query_tags & chunk.tags:
                matched_chunks.append(chunk)
                print(f"   Matched base behavior: ƒêi·ªÅu {chunk.article} kho·∫£n {chunk.khoan} ƒëi·ªÉm {chunk.diem}")
        
        # SECOND: Add escalation chunks
        for esc_idx in self.escalation_chunks:
            esc_chunk = self.chunks[esc_idx]
            
            # ALWAYS filter by vehicle article first
            if esc_chunk.article != vehicle_article:
                continue
            
            # If we have target khoan from speed detection, also check that
            if target_khoan and esc_chunk.khoan != target_khoan:
                continue
            
            ref_match = bool(behavior_refs & esc_chunk.escalation_refs)
            tag_match = bool(query_tags & esc_chunk.tags)
            tai_nan_match = has_tai_nan and esc_chunk.priority == 100
            
            if ref_match or tag_match or tai_nan_match:
                matched_chunks.append(esc_chunk)
                print(f"   Matched escalation: ƒêi·ªÅu {esc_chunk.article} kho·∫£n {esc_chunk.khoan} (priority={esc_chunk.priority})")
        
        if matched_chunks:
            # STRATEGY: Select chunk with best match
            # Priority:
            # 1. For accidents (tai n·∫°n): Highest penalty (escalation)
            # 2. For normal violations: Best text match + appropriate penalty
            
            # Filter out chunks with no penalty info
            chunks_with_penalty = [c for c in matched_chunks if c.penalty_max]
            
            # Check for exact phrase matches in query
            # This helps distinguish "kh√¥ng g·∫Øn bi·ªÉn s·ªë" from "g·∫Øn bi·ªÉn s·ªë kh√¥ng ƒë√∫ng"
            exact_matches = []
            for chunk in chunks_with_penalty:
                chunk_text = chunk.content.lower()
                # Check if key phrases from query appear in chunk
                query_phrases = []
                if "kh√¥ng g·∫Øn bi·ªÉn s·ªë" in query_lower or "kh√¥ng c√≥ bi·ªÉn s·ªë" in query_lower:
                    query_phrases = ["kh√¥ng g·∫Øn bi·ªÉn s·ªë"]
                elif "g·∫Øn bi·ªÉn s·ªë kh√¥ng ƒë√∫ng" in query_lower or "bi·ªÉn s·ªë gi·∫£" in query_lower:
                    query_phrases = ["g·∫Øn bi·ªÉn s·ªë kh√¥ng ƒë√∫ng", "kh√¥ng do c∆° quan"]
                
                if query_phrases:
                    for phrase in query_phrases:
                        if phrase in chunk_text:
                            exact_matches.append(chunk)
                            break
            
            # Context-based location filtering for violations
            # Default: prefer general violations over tunnel-specific unless explicitly mentioned
            has_tunnel_keyword = "trong h·∫ßm" in query_lower or "h·∫ßm ƒë∆∞·ªùng" in query_lower
            has_road_keyword = any(road_kw in query_lower for road_kw in ["ngo√†i ƒë∆∞·ªùng", "tr√™n ƒë∆∞·ªùng", "ƒë∆∞·ªùng ph·ªë", "ban ƒë√™m", "ban ng√†y"])
            
            if has_tunnel_keyword:
                # Explicitly mentions tunnel - prefer tunnel violations
                tunnel_chunks = [c for c in chunks_with_penalty if "h·∫ßm" in c.content.lower()]
                if tunnel_chunks:
                    chunks_with_penalty = tunnel_chunks
                    exact_matches = [c for c in exact_matches if "h·∫ßm" in c.content.lower()]
                    print(f"   Preferred tunnel-specific violations (query mentions tunnel)")
            elif has_road_keyword or not has_tunnel_keyword:
                # Explicitly mentions road OR no location context - exclude tunnel violations (default to general)
                chunks_with_penalty = [c for c in chunks_with_penalty 
                                      if "h·∫ßm" not in c.content.lower() and "h·∫ßm ƒë∆∞·ªùng b·ªô" not in c.content.lower()]
                exact_matches = [c for c in exact_matches 
                               if "h·∫ßm" not in c.content.lower() and "h·∫ßm ƒë∆∞·ªùng b·ªô" not in c.content.lower()]
                reason = "road" if has_road_keyword else "default to general case"
                print(f"   Excluded tunnel-specific violations ({reason})")
            
            # If we have exact matches and no accident, prefer those over highest penalty
            if exact_matches and not has_tai_nan:
                # Among exact matches, prefer the article that matches vehicle type
                # For xe √¥ t√¥: prefer ƒêi·ªÅu 13 (car equipment) over ƒêi·ªÅu 16 (other vehicles)
                # For xe m√¥ t√¥: prefer ƒêi·ªÅu 14 (motorcycle equipment)
                preferred_article = 13 if vehicle_article == 6 else 14
                
                vehicle_specific = [c for c in exact_matches if c.article == preferred_article]
                if vehicle_specific:
                    # Sort by penalty to get the base violation (lower penalty)
                    vehicle_specific.sort(key=lambda c: c.penalty_max or 0)
                    primary_chunk = vehicle_specific[0]
                    print(f"   Selected exact match for vehicle article {preferred_article}: ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
                else:
                    # No vehicle-specific match, use any exact match (lowest penalty)
                    exact_matches.sort(key=lambda c: c.penalty_max or 0)
                    primary_chunk = exact_matches[0]
                    print(f"   Selected exact match: ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
            
            elif has_tai_nan and chunks_with_penalty:
                # Special case: Check for kho·∫£n 13 (reckless driving + accident)
                # Kho·∫£n 13 is for kho·∫£n 12 violations (l·∫°ng l√°ch, ƒë√°nh v√µng) causing accidents
                khoan_13_chunks = [c for c in chunks_with_penalty if c.khoan == 13]
                khoan_12_chunks = [c for c in chunks_with_penalty if c.khoan == 12]
                
                # If both kho·∫£n 12 and kho·∫£n 13 are matched, prefer kho·∫£n 13 (accident escalation of kho·∫£n 12)
                if khoan_13_chunks and khoan_12_chunks:
                    primary_chunk = khoan_13_chunks[0]
                    print(f"   Selected kho·∫£n 13 (reckless driving + accident): ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
                else:
                    # Normal accident escalation logic
                    # For accidents, prefer escalation chunks (is_escalation=True) first
                    # This ensures kho·∫£n 10 (escalation, 20M-22M) is selected over other base behaviors
                    escalation_chunks = [c for c in chunks_with_penalty if c.is_escalation]
                    base_chunks = [c for c in chunks_with_penalty if not c.is_escalation]
                    
                    if escalation_chunks:
                        # If there are kho·∫£n 10 chunks, check if they have references matching base behaviors
                        khoan_10_chunks = [c for c in escalation_chunks if c.khoan == 10]
                        
                        if len(khoan_10_chunks) > 1:
                            # Try to select the most specific one based on references
                            # Check which base behaviors were matched
                            base_khoan_diem = set()
                            for chunk in base_chunks:
                                if chunk.diem:
                                    base_khoan_diem.add((chunk.khoan, chunk.diem))
                            
                            # Check which kho·∫£n 10 chunk's references match base behaviors
                            best_match = None
                            for chunk in khoan_10_chunks:
                                if chunk.references:
                                    # kho·∫£n 10 ƒëi·ªÉm b has many references, ƒëi·ªÉm a has none
                                    # If we matched behaviors from kho·∫£n 1-9, prefer ƒëi·ªÉm b
                                    if base_khoan_diem:
                                        for ref in chunk.references:
                                            ref_tuple = (ref.get('khoan'), ref.get('diem'))
                                            if ref_tuple in base_khoan_diem:
                                                best_match = chunk
                                                print(f"   Found reference match: kho·∫£n {ref['khoan']} ƒëi·ªÉm {ref['diem']}")
                                                break
                                    if best_match:
                                        break
                            
                            if best_match:
                                primary_chunk = best_match
                            else:
                                # Sort by priority first, then penalty
                                escalation_chunks.sort(key=lambda x: (x.priority, x.penalty_max), reverse=True)
                                primary_chunk = escalation_chunks[0]
                        else:
                            # Sort escalations by priority first, then penalty
                            escalation_chunks.sort(key=lambda x: (x.priority, x.penalty_max), reverse=True)
                            primary_chunk = escalation_chunks[0]
                        
                        print(f"   Selected tai n·∫°n escalation (is_escalation=True): ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ƒëi·ªÉm {primary_chunk.diem or 'None'} ({primary_chunk.penalty_max//1000}K)")
                    else:
                        # Fallback to base chunks sorted by penalty
                        base_chunks.sort(key=lambda x: x.penalty_max, reverse=True)
                        primary_chunk = base_chunks[0]
                        print(f"   Selected tai n·∫°n base chunk: ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
            else:
                # No tai n·∫°n - normal violation
                # If we already found exact matches via phrase matching, use them (set earlier)
                if 'primary_chunk' not in locals():
                    # PRIORITIZE SPECIFIC TAGS over general ones
                    # Define specific tag hierarchy (more specific first)
                    specific_tags = [
                        'co_vu_dua_xe',         # Most specific: supporting/gathering for racing (1-2M)
                        'khong_mang_bang_lai',  # Most specific: not carrying license (200K-300K)
                        'khong_bang_lai',       # Specific: not having license (2-4M, 6-8M)
                        'khong_bien_so',        # Specific: no license plate
                        'dua_xe',               # Specific: racing (40-50M or confiscation)
                        'khong_giay_to',        # General: no documents
                    ]
                    
                    # Try to filter by most specific tag first
                    filtered_by_specific_tag = chunks_with_penalty
                    for specific_tag in specific_tags:
                        if specific_tag in query_tags:
                            # Filter to only chunks with this specific tag
                            tag_filtered = [c for c in chunks_with_penalty if specific_tag in c.tags]
                            if tag_filtered:
                                filtered_by_specific_tag = tag_filtered
                                print(f"   Filtered to {len(tag_filtered)} chunks with specific tag '{specific_tag}'")
                                
                                # ADDITIONAL FILTERING: For cross-cutting articles, filter by vehicle type in text
                                # Skip vehicle filtering for ƒêi·ªÅu 35 (racing) - applies to all vehicles
                                if vehicle_article in [6, 7] and not any(c.article == 35 for c in tag_filtered):
                                    vehicle_specific = []
                                    for chunk in tag_filtered:
                                        chunk_text = chunk.content.lower()
                                        # Check if chunk mentions the correct vehicle type
                                        if vehicle_article == 6:  # Cars
                                            if 'xe √¥ t√¥' in chunk_text or '√¥ t√¥' in chunk_text:
                                                vehicle_specific.append(chunk)
                                        elif vehicle_article == 7:  # Motorcycles
                                            if 'xe m√¥ t√¥' in chunk_text or 'm√¥ t√¥' in chunk_text:
                                                vehicle_specific.append(chunk)
                                            else:
                                                print(f"      - Filtered OUT ƒêi·ªÅu {chunk.article} k{chunk.khoan} ƒëi·ªÉm {chunk.diem}: no 'xe m√¥ t√¥' in text")
                                    
                                    if vehicle_specific:
                                        filtered_by_specific_tag = vehicle_specific
                                        print(f"   Further filtered to {len(vehicle_specific)} chunks matching vehicle type (ƒêi·ªÅu {vehicle_article})")
                                
                                break  # Use the most specific tag match
                    
                    # SPECIAL CASE: If filtered by 'khong_mang_bang_lai' but query has NO 'van_tai',
                    # and all results are business transport (van_tai), fall back to 'khong_bang_lai'
                    # because for individuals, "kh√¥ng mang" is treated same as "kh√¥ng c√≥" in the law
                    if ('khong_mang_bang_lai' in query_tags and 'van_tai' not in query_tags and 
                        filtered_by_specific_tag and all('van_tai' in c.tags for c in filtered_by_specific_tag)):
                        print(f"   All 'khong_mang_bang_lai' results are business transport, falling back to 'khong_bang_lai'")
                        # Retry with khong_bang_lai tag - search in ALL matched_chunks, not just chunks_with_penalty
                        tag_filtered = [c for c in matched_chunks if 'khong_bang_lai' in c.tags]
                        if tag_filtered:
                            filtered_by_specific_tag = tag_filtered
                            print(f"   Filtered to {len(tag_filtered)} chunks with 'khong_bang_lai' tag")
                            for c in tag_filtered:
                                print(f"      - ƒêi·ªÅu {c.article} k{c.khoan} ƒëi·ªÉm {c.diem}: {c.penalty_min}-{c.penalty_max}")
                            
                            # Apply vehicle filtering again
                            if vehicle_article in [6, 7]:
                                vehicle_specific = []
                                for chunk in tag_filtered:
                                    chunk_text = chunk.content.lower()
                                    if vehicle_article == 6 and ('xe √¥ t√¥' in chunk_text or '√¥ t√¥' in chunk_text):
                                        vehicle_specific.append(chunk)
                                    elif vehicle_article == 7 and ('xe m√¥ t√¥' in chunk_text or 'm√¥ t√¥' in chunk_text):
                                        vehicle_specific.append(chunk)
                                
                                if vehicle_specific:
                                    filtered_by_specific_tag = vehicle_specific
                                    print(f"   Further filtered to {len(vehicle_specific)} chunks matching vehicle type")
                    
                    # ENGINE SIZE FILTERING: If engine size detected, filter to matching provisions
                    if engine_size_tag and filtered_by_specific_tag:
                        engine_filtered = [c for c in filtered_by_specific_tag if engine_size_tag in c.tags]
                        if engine_filtered:
                            filtered_by_specific_tag = engine_filtered
                            print(f"   Further filtered to {len(engine_filtered)} chunks matching engine size ({engine_size_tag})")
                    
                    # SPECIAL CASE: If van_tai tag is detected, prefer chunks with van_tai tag (business transport)
                    # Business transport usually has LOWER penalties than individual violations
                    if 'van_tai' in query_tags and filtered_by_specific_tag:
                        van_tai_chunks = [c for c in filtered_by_specific_tag if 'van_tai' in c.tags]
                        if van_tai_chunks:
                            filtered_by_specific_tag = van_tai_chunks
                            print(f"   Further filtered to {len(van_tai_chunks)} business transport chunks (van_tai)")
                            # For business transport, prefer LOWEST penalty (not highest)
                            filtered_by_specific_tag.sort(key=lambda x: x.penalty_max)
                            primary_chunk = filtered_by_specific_tag[0]
                            print(f"   Selected business transport chunk with lowest penalty: ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
                    
                    # For normal queries without exact match
                    if 'primary_chunk' not in locals():
                        if filtered_by_specific_tag:
                            # SPECIAL: For "kh√¥ng mang" queries (not carrying license), prefer LOWEST penalty
                            # as conservative default (without engine size info, assume lower tier)
                            if 'khong_mang_bang_lai' in query_tags and 'van_tai' not in query_tags:
                                # Individual not carrying license - prefer lower penalty tier
                                filtered_by_specific_tag.sort(key=lambda x: x.penalty_max)
                                primary_chunk = filtered_by_specific_tag[0]
                                print(f"   Selected chunk with lowest penalty (kh√¥ng mang - individual): ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
                            else:
                                # For other queries, prefer highest penalty (most specific)
                                filtered_by_specific_tag.sort(key=lambda x: x.penalty_max, reverse=True)
                                primary_chunk = filtered_by_specific_tag[0]
                                print(f"   Selected chunk with highest penalty: ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan} ({primary_chunk.penalty_max//1000}K)")
                        else:
                            # Fallback to highest priority
                            matched_chunks.sort(key=lambda x: x.priority, reverse=True)
                            primary_chunk = matched_chunks[0]
                            print(f"   Selected by priority {primary_chunk.priority}")
        else:
            # No escalations matched, use first behavior chunk
            primary_chunk = behavior_chunks[0]
            print(f"   No escalation, using base behavior chunk")
        
        penalty_info = None
        if primary_chunk.penalty_min or primary_chunk.penalty_max:
            penalty_info = {
                "min": primary_chunk.penalty_min,
                "max": primary_chunk.penalty_max,
                "text": self._format_penalty(primary_chunk.penalty_min, primary_chunk.penalty_max)
            }
        
        suspension_info = None
        if primary_chunk.license_suspension_months:
            min_months, max_months = primary_chunk.license_suspension_months
            suspension_info = {
                "min_months": min_months,
                "max_months": max_months,
                "text": f"T∆∞·ªõc GPLX t·ª´ {min_months} ƒë·∫øn {max_months} th√°ng"
            }
        
        return {
            "status": "success",
            "primary_chunk": {
                "reference": f"ƒêi·ªÅu {primary_chunk.article} kho·∫£n {primary_chunk.khoan}" + 
                            (f" ƒëi·ªÉm {primary_chunk.diem}" if primary_chunk.diem else ""),
                "content": primary_chunk.content,
                "tags": list(primary_chunk.tags),
                "is_escalation": primary_chunk.is_escalation,
                "priority": primary_chunk.priority,
                "penalty": penalty_info,
                "point_deduction": primary_chunk.point_deduction,
                "license_suspension": suspension_info
            },
            "related_chunks": [
                {
                    "reference": f"ƒêi·ªÅu {chunk.article} kho·∫£n {chunk.khoan}" + 
                                (f" ƒëi·ªÉm {chunk.diem}" if chunk.diem else ""),
                    "content": chunk.content[:200] + "...",
                    "tags": list(chunk.tags),
                    "point_deduction": chunk.point_deduction
                }
                for chunk in behavior_chunks[:3] if chunk != primary_chunk
            ],
            "escalations_applied": len([c for c in matched_chunks if c.is_escalation])
        }
    
    def _format_penalty(self, min_val: Optional[int], max_val: Optional[int]) -> str:
        """Format penalty amount in Vietnamese"""
        if not min_val and not max_val:
            return "Ch∆∞a x√°c ƒë·ªãnh"
        
        def format_amount(val):
            if val >= 1000:
                return f"{val:,}ƒë"
            else:
                return f"{val:,}ƒë"
        
        if min_val and max_val:
            return f"{format_amount(min_val)} - {format_amount(max_val)}"
        elif min_val:
            return f"T·ª´ {format_amount(min_val)}"
        else:
            return f"ƒê·∫øn {format_amount(max_val)}"


# Test function
def test_with_points():
    """Test the enhanced RAG with point deduction"""
    rag = TrafficLawRAGWithPoints(r"D:\crawl_law\nd168_metadata_clean.json")
    
    test_cases = [
        "Ch·∫°y qu√° t·ªëc ƒë·ªô 25km/h tr√™n cao t·ªëc m·ª©c ph·∫°t ra sao?",
        "Xe √¥ t√¥ r·∫Ω ph·∫£i sai quy ƒë·ªãnh g√¢y tai n·∫°n th√¨ ph·∫°t nh∆∞ th·∫ø n√†o?",
        "V∆∞·ª£t ƒë√®n ƒë·ªè b·ªã ph·∫°t bao nhi√™u v√† tr·ª´ m·∫•y ƒëi·ªÉm?",
        "C√°n v·∫°ch k·∫ª ƒë∆∞·ªùng gi·ªØa 2 l√†n xe c√≥ nguy hi·ªÉm kh√¥ng?"
    ]
    
    for query in test_cases:
        result = rag.retrieve(query)
        
        print(f"\n{'='*80}")
        print(f"Query: {query}")
        print(f"{'='*80}")
        
        if result["status"] == "success":
            primary = result["primary_chunk"]
            print(f"\nüìú Reference: {primary['reference']}")
            print(f"üí∞ Penalty: {primary['penalty']['text'] if primary['penalty'] else 'N/A'}")
            print(f"üìâ Point Deduction: {primary['point_deduction']} ƒëi·ªÉm" if primary['point_deduction'] else "üìâ Point Deduction: None")
            print(f"üö´ License Suspension: {primary['license_suspension']['text']}" if primary['license_suspension'] else "üö´ License Suspension: None")
            print(f"‚ö†Ô∏è Priority: {primary['priority']}")
            print(f"\nüìù Content: {primary['content'][:200]}...")
        else:
            print(f"‚ùå {result['message']}")


if __name__ == "__main__":
    test_with_points()
