# ğŸš€ HÆ°á»›ng Dáº«n Deploy LÃªn Render

## ğŸ“‹ Tá»•ng Quan

Dá»± Ã¡n Ä‘Ã£ Ä‘Æ°á»£c tÃ¡ch thÃ nh 2 pháº§n:
- **Backend**: FastAPI service (Python)
- **Frontend**: Static website (HTML/CSS/JS)

## ğŸ—ï¸ Cáº¥u TrÃºc Dá»± Ãn

```
.
â”œâ”€â”€ backend/              # Backend API
â”‚   â”œâ”€â”€ app.py           # FastAPI application
â”‚   â”œâ”€â”€ inference.py      # AI model inference
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # Frontend static files
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â”œâ”€â”€ chat.js
â”‚   â”œâ”€â”€ config.js        # API configuration
â”‚   â””â”€â”€ img/            # Images
â”œâ”€â”€ qwen3-0.6B-instruct-trafficlaws/  # Model checkpoint
â””â”€â”€ render.yaml          # Render configuration
```

## ğŸ”§ Deploy Backend LÃªn Render

### BÆ°á»›c 1: Táº¡o Web Service trÃªn Render

1. ÄÄƒng nháº­p vÃ o [Render](https://render.com)
2. Click **"New +"** â†’ **"Web Service"**
3. Káº¿t ná»‘i repository GitHub/GitLab cá»§a báº¡n
4. Cáº¥u hÃ¬nh:
   - **Name**: `legal-ai-backend`
   - **Environment**: `Python 3`
   - **Build Command**: `pip install -r backend/requirements.txt`
   - **Start Command**: `cd backend && python app.py`
   - **Plan**: Chá»n plan phÃ¹ há»£p (Starter/Standard)

### BÆ°á»›c 2: Environment Variables

ThÃªm cÃ¡c biáº¿n mÃ´i trÆ°á»ng:

```
PORT=8000
HOST=0.0.0.0
MODEL_HF_REPO=sigmaloop/qwen3-0.6B-instruct-trafficlaws
MODEL_HF_SUBFOLDER=model
HF_TOKEN=your_huggingface_token_here
# Hoáº·c dÃ¹ng local path (náº¿u khÃ´ng dÃ¹ng Hugging Face):
# MODEL_PATH=../qwen3-0.6B-instruct-trafficlaws/model
CORS_ORIGINS=https://your-frontend-url.onrender.com
DATABASE_URL=sqlite:///./feedback.db
```

**LÆ°u Ã½ vá» Model:**
- **Khuyáº¿n nghá»‹**: DÃ¹ng Hugging Face Hub - Model Ä‘Ã£ cÃ³ táº¡i: `sigmaloop/qwen3-0.6B-instruct-trafficlaws`
- Set `MODEL_HF_REPO=sigmaloop/qwen3-0.6B-instruct-trafficlaws` vÃ  `MODEL_HF_SUBFOLDER=model`
- **Náº¿u repo lÃ  Private**: Cáº§n set `HF_TOKEN=your_huggingface_token` Ä‘á»ƒ authenticate
  - Láº¥y token táº¡i: https://huggingface.co/settings/tokens
  - Táº¡o token vá»›i quyá»n "Read" lÃ  Ä‘á»§
- **Alternative**: DÃ¹ng local path - set `MODEL_PATH=../qwen3-0.6B-instruct-trafficlaws/model`
- Model sáº½ tá»± Ä‘á»™ng download tá»« Hugging Face Hub khi deploy (láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt)

**LÆ°u Ã½ vá» Database:**
- **Development**: Sá»­ dá»¥ng SQLite (máº·c Ä‘á»‹nh) - file `feedback.db` sáº½ Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng
- **Production**: NÃªn dÃ¹ng PostgreSQL (táº¡o PostgreSQL database trÃªn Render vÃ  set `DATABASE_URL`)

**LÆ°u Ã½**: 
- `CORS_ORIGINS` nÃªn set thÃ nh URL frontend cá»§a báº¡n
- Náº¿u deploy cÃ¹ng domain, cÃ³ thá»ƒ dÃ¹ng `*` (khÃ´ng khuyáº¿n nghá»‹ cho production)

### BÆ°á»›c 3: Upload Model Checkpoint

**Khuyáº¿n nghá»‹: Sá»­ dá»¥ng Hugging Face Hub** (Dá»… nháº¥t!)

Xem hÆ°á»›ng dáº«n chi tiáº¿t trong [UPLOAD_TO_HUGGINGFACE.md](./UPLOAD_TO_HUGGINGFACE.md)

**Quick steps:**
1. Upload model lÃªn Hugging Face Hub
2. Set environment variable: `MODEL_HF_REPO=your-username/repo-name`
3. Model sáº½ tá»± Ä‘á»™ng download khi deploy

**CÃ¡c cÃ¡ch khÃ¡c:**

#### CÃ¡ch 2: Sá»­ dá»¥ng Git LFS
```bash
# CÃ i Git LFS
git lfs install

# Track model files
git lfs track "qwen3-0.6B-instruct-trafficlaws/**"

# Commit vÃ  push
git add .gitattributes
git add qwen3-0.6B-instruct-trafficlaws/
git commit -m "Add model with LFS"
git push
```

#### CÃ¡ch 3: Sá»­ dá»¥ng External Storage
- Upload model lÃªn S3/Google Cloud Storage
- Download trong build command:
```bash
# ThÃªm vÃ o build command
wget https://your-storage-url/model.zip && unzip model.zip
```

### BÆ°á»›c 4: Deploy

1. Click **"Create Web Service"**
2. Render sáº½ tá»± Ä‘á»™ng build vÃ  deploy
3. LÆ°u láº¡i URL backend (vÃ­ dá»¥: `https://legal-ai-backend.onrender.com`)

## ğŸŒ Deploy Frontend

### Option 1: Static Site trÃªn Render

1. Táº¡o **Static Site** trÃªn Render
2. Cáº¥u hÃ¬nh:
   - **Build Command**: `echo "No build needed"`
   - **Publish Directory**: `frontend`
3. ThÃªm Environment Variable:
   ```
   API_BASE_URL=https://your-backend-url.onrender.com
   ```
4. Cáº­p nháº­t `frontend/config.js` Ä‘á»ƒ Ä‘á»c tá»« environment variable

### Option 2: Sá»­ dá»¥ng Static Hosting KhÃ¡c

#### Vercel:
```bash
npm i -g vercel
cd frontend
vercel
```

#### Netlify:
1. KÃ©o tháº£ thÆ° má»¥c `frontend` vÃ o Netlify
2. Set build command: `echo "No build needed"`
3. Set publish directory: `frontend`

#### GitHub Pages:
1. Push code lÃªn GitHub
2. Settings â†’ Pages
3. Source: `frontend` folder

### Cáº¥u HÃ¬nh API URL

Sau khi deploy frontend, cáº§n cáº­p nháº­t API URL:

**CÃ¡ch 1**: Sá»­ dá»¥ng `config.js` (Ä‘Ã£ cÃ³ sáºµn)
- Frontend tá»± Ä‘á»™ng detect API URL tá»« `window.API_BASE_URL`
- CÃ³ thá»ƒ set trong build process

**CÃ¡ch 2**: Sá»­ dá»¥ng environment variable
- ThÃªm script vÃ o `index.html`:
```html
<script>
  window.API_BASE_URL = 'https://your-backend-url.onrender.com';
</script>
```

## ğŸ” Cáº¥u HÃ¬nh CORS

Sau khi cÃ³ URL frontend, cáº­p nháº­t CORS trong backend:

1. VÃ o Render Dashboard â†’ Backend Service â†’ Environment
2. Cáº­p nháº­t `CORS_ORIGINS`:
```
CORS_ORIGINS=https://your-frontend-url.onrender.com,https://your-custom-domain.com
```

## ğŸ“Š Monitoring & Logs

### Xem Logs:
- Render Dashboard â†’ Service â†’ Logs
- Hoáº·c dÃ¹ng Render CLI:
```bash
render logs --service legal-ai-backend
```

### Health Check:
- Backend health endpoint: `https://your-backend-url.onrender.com/health`
- Kiá»ƒm tra model Ä‘Ã£ load: Xem logs

## âš ï¸ LÆ°u Ã Quan Trá»ng

### 1. Model Size
- Model checkpoint cÃ³ thá»ƒ ráº¥t lá»›n (>1GB)
- Render free tier cÃ³ giá»›i háº¡n
- CÃ¢n nháº¯c sá»­ dá»¥ng Git LFS hoáº·c external storage

### 2. Memory Requirements
- Model 0.6B cáº§n ~2-4GB RAM
- Render Starter plan: 512MB RAM (cÃ³ thá»ƒ khÃ´ng Ä‘á»§)
- Khuyáº¿n nghá»‹: Standard plan (2GB RAM) hoáº·c cao hÆ¡n

### 3. Cold Start
- Render free tier cÃ³ cold start (~30s-2min)
- Model loading máº¥t thÃªm thá»i gian
- CÃ¢n nháº¯c upgrade plan Ä‘á»ƒ trÃ¡nh sleep

### 4. Timeout
- Render cÃ³ timeout limit
- Model inference cÃ³ thá»ƒ máº¥t >30s
- CÃ¢n nháº¯c tÄƒng timeout hoáº·c optimize model

## ğŸ› Troubleshooting

### Lá»—i: Model khÃ´ng load Ä‘Æ°á»£c
- Kiá»ƒm tra `MODEL_PATH` environment variable
- Kiá»ƒm tra logs Ä‘á»ƒ xem Ä‘Æ°á»ng dáº«n model
- Äáº£m báº£o model files Ä‘Ã£ Ä‘Æ°á»£c upload

### Lá»—i: CORS
- Kiá»ƒm tra `CORS_ORIGINS` environment variable
- Äáº£m báº£o frontend URL Ä‘Ãºng format
- Thá»­ set `*` táº¡m thá»i Ä‘á»ƒ test

### Lá»—i: Out of Memory
- Upgrade plan lÃªn Standard hoáº·c cao hÆ¡n
- Sá»­ dá»¥ng quantization (8-bit)
- Giáº£m `max_new_tokens` trong inference.py

### Lá»—i: Timeout
- TÄƒng timeout trong Render settings
- Optimize model loading
- Sá»­ dá»¥ng response caching

## ğŸ“ Checklist Deploy

- [ ] Backend service Ä‘Ã£ táº¡o vÃ  deploy thÃ nh cÃ´ng
- [ ] Model checkpoint Ä‘Ã£ Ä‘Æ°á»£c upload
- [ ] Environment variables Ä‘Ã£ set Ä‘Ãºng
- [ ] Database Ä‘Ã£ Ä‘Æ°á»£c setup (SQLite hoáº·c PostgreSQL)
- [ ] Frontend Ä‘Ã£ deploy vÃ  cÃ³ thá»ƒ truy cáº­p
- [ ] API URL Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh trong frontend
- [ ] CORS Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng
- [ ] Health check endpoint hoáº¡t Ä‘á»™ng
- [ ] Test chat functionality
- [ ] Test feedback functionality

## ğŸ‰ HoÃ n ThÃ nh!

Sau khi deploy xong, báº¡n sáº½ cÃ³:
- Backend API: `https://your-backend.onrender.com`
- Frontend: `https://your-frontend.onrender.com`

Truy cáº­p frontend URL Ä‘á»ƒ sá»­ dá»¥ng á»©ng dá»¥ng!

## ğŸ“ Há»— Trá»£

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra logs trÃªn Render Dashboard
2. Xem [OPTIMIZATION_GUIDE.md](./OPTIMIZATION_GUIDE.md) Ä‘á»ƒ tá»‘i Æ°u
3. Kiá»ƒm tra [Render Documentation](https://render.com/docs)

